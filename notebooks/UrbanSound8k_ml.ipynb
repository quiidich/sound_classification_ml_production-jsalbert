{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UrbanSound8k_machine_learning.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAR2KCeZGVX3"
      },
      "source": [
        "# Applying Machine Learning on UrbanSound8k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USPAMetFGQ7O"
      },
      "source": [
        "## Install Packages\n",
        "\n",
        "We install:\n",
        "- Machine learning libraries: `Keras`, `sklearn`\n",
        "- Audio processing: `librosa`\n",
        "- Plots: `Plotly`, `matplotlib`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sefSvCUtzKfx",
        "outputId": "e9dc1df1-b0ef-4d23-b934-fba0da285638"
      },
      "source": [
        "!pip install pandas\n",
        "!pip install setuptools\n",
        "!pip install numpy\n",
        "!pip install sklearn\n",
        "!pip install librosa\n",
        "!pip install plotly\n",
        "!pip install matplotlib\n",
        "!pip install pillow\n",
        "!pip install keras"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post11.tar.gz (3.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.5.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.7)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (4.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (23.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.7.22)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (23.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrVjHhS40MZM"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import librosa\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from PIL import Image"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NA_prGkx8N7P",
        "outputId": "b8297e4a-f507-4b9a-9dbf-6a166e46f8fb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd gdrive/MyDrive/urbansound8k\n",
        "!wget https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz -O urban8k.tgz"
      ],
      "metadata": {
        "id": "NoPrdeZSQn6H",
        "outputId": "1fd25863-1e7c-447a-ae47-2aa4347b0e21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "--2023-11-27 01:11:05--  https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz\n",
            "Resolving zenodo.org (zenodo.org)... 188.184.103.159, 188.184.98.238, 188.185.79.172, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.184.103.159|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/1203745/files/UrbanSound8K.tar.gz [following]\n",
            "--2023-11-27 01:11:06--  https://zenodo.org/records/1203745/files/UrbanSound8K.tar.gz\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6023741708 (5.6G) [application/octet-stream]\n",
            "Saving to: ‘urban8k.tgz’\n",
            "\n",
            "urban8k.tgz         100%[===================>]   5.61G  23.6MB/s    in 4m 46s  \n",
            "\n",
            "2023-11-27 01:15:53 (20.1 MB/s) - ‘urban8k.tgz’ saved [6023741708/6023741708]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ELY9HvKQiwv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz -O urban8k.tgz"
      ],
      "metadata": {
        "id": "_IdovGFEPado",
        "outputId": "a800e4ae-e3c5-4199-c12a-452052b18818",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-27 00:52:26--  https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.79.172, 188.184.98.238, 188.184.103.159, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.79.172|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/1203745/files/UrbanSound8K.tar.gz [following]\n",
            "--2023-11-27 00:52:26--  https://zenodo.org/records/1203745/files/UrbanSound8K.tar.gz\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6023741708 (5.6G) [application/octet-stream]\n",
            "Saving to: ‘urban8k.tgz’\n",
            "\n",
            "urban8k.tgz         100%[===================>]   5.61G  21.9MB/s    in 4m 35s  \n",
            "\n",
            "2023-11-27 00:57:01 (20.9 MB/s) - ‘urban8k.tgz’ saved [6023741708/6023741708]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "try:\n",
        "    dataset_path = '/content/gdrive/MyDrive/urbansound8k'\n",
        "    shutil.copy('urban8k.tgz', os.path.join(dataset_path, 'urban8k.tgz'))\n",
        "except Exception as err:\n",
        "    print(str(err))\n",
        "\n",
        "#!tar -xzf urban8k.tgz\n",
        "#!rm urban8k.tgz"
      ],
      "metadata": {
        "id": "MA2Na1D1_LhZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ikg2sdn36H0"
      },
      "source": [
        "# # Unzip dataset\n",
        "# !wget https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz -O urban8k.tgz\n",
        "!tar -xzf urban8k.tgz\n",
        "!rm urban8k.tgz"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOx82BdSom6q"
      },
      "source": [
        "## Design Choices and Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YbZ0ZqyosFi"
      },
      "source": [
        "After analysing the dataset and spending a bit of time reading about state-of-the-art on audio signal classification, and some of my [previous work](https://github.com/jsalbert/Music-Genre-Classification-with-Deep-Learning) I have made the following design choices and proposals:\n",
        "\n",
        "Train a Convolutional Neural Network and use either MFCCs, STFT or Mel-Spectogram as input.\n",
        "\n",
        "- As the audios duration range from 0 to 4s, I pad the spectogram generated, to make all the audios of equal length.\n",
        "\n",
        "Feature options:\n",
        "\n",
        "- Using MFCCs as features:\n",
        "  - It is usual to compute the first 13 MFCCs, their derivatives and second derivatives and use it as features.\n",
        "  - Or it is also usual to use 40 MFCCs as it is the Librosa default.\n",
        "\n",
        "- Using the STFT as features:\n",
        "  - Contains less human processing than MFCCs and Mel-Spectogram, the CNN could learn other filters rather than the representations designed by humans.\n",
        "\n",
        "- Using Mel-Spectogram as features:\n",
        "  - A transformation applied on the STFT that approximates how humans perceive the sound. Less human engineered than MFCCs but a bit more than STFT.\n",
        "\n",
        "My first choice would be using STFT and Mel-Spectogram as it looks that CNNs could take more advantage of the frequency-temporal structure but due to **computational resources** and limited time I will show the use **MFCCs** as features as they are much more memory efficient.\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctXVt_OEsrie"
      },
      "source": [
        "## Dataset Preprocessing and Splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J728zt898gNW"
      },
      "source": [
        "I load all the audio data using Librosa and the default sample rate of 22050Hz. This design decision is based on\n",
        "([Source]((https://librosa.org/blog/2019/07/17/resample-on-load/#Okay...-but-why-22050-Hz?--Why-not-44100-or-48000?))) and in further experiments different sample rates could be tried.\n",
        "\n",
        "> Humans can hear up to around 20000 Hz, it's possible to successfully analyze music and speech data at much lower rates without sacrificing much. The highest pitches we usually care about detecting are around C9≈8372 Hz, well below the 11025 cutoff implied by fs=22050.\n",
        "\n",
        "By default Librosa will load the audio in mono, giving us 1 channel.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPbaPrONsoiY"
      },
      "source": [
        "# FeatureExtractor class including librosa audio processing functions\n",
        "class FeatureExtractor:\n",
        "    def __init__(self, csv_file):\n",
        "        self.csv_file = csv_file\n",
        "        self.max_audio_duration = 4\n",
        "        self.dataset_df = self._create_dataset(csv_file)\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_dataset(csv_file):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataset_path: path with the .wav files after unzipping\n",
        "        Returns: A pandas dataframe with the list of files and labels (`filenames`, `labels`)\n",
        "        \"\"\"\n",
        "        dataset_df = pd.read_csv(csv_file)\n",
        "        filepaths = []\n",
        "        for i, row in dataset_df.iterrows():\n",
        "            filepaths.append(os.path.join('UrbanSound8K/audio', 'fold'+str(row['fold']), row['slice_file_name']))\n",
        "        dataset_df['filepath'] = filepaths\n",
        "        return dataset_df\n",
        "\n",
        "    @staticmethod\n",
        "    def _compute_max_pad_length(max_audio_length, sample_rate=22050, n_fft=2048, hop_length=512):\n",
        "        dummy_file = np.random.random(max_audio_length*sample_rate)\n",
        "        stft = librosa.stft(dummy_file, n_fft=n_fft, hop_length=hop_length)\n",
        "        # Return an even number for CNN computation purposes\n",
        "        if stft.shape[1] % 2 != 0:\n",
        "            return stft.shape[1] + 1\n",
        "        return stft.shape[1]\n",
        "\n",
        "    def compute_save_features(self,\n",
        "                        mode='mfcc',\n",
        "                        sample_rate=22050,\n",
        "                        n_fft=2048,\n",
        "                        hop_length=512,\n",
        "                        n_mfcc=40,\n",
        "                        output_path='features',\n",
        "                        deltas=False\n",
        "                        ):\n",
        "        dataset_features = []\n",
        "        max_pad = self._compute_max_pad_length(self.max_audio_duration,\n",
        "                                               sample_rate=sample_rate,\n",
        "                                               n_fft=n_fft,\n",
        "                                               hop_length=hop_length)\n",
        "        print('Max Padding = ', max_pad)\n",
        "\n",
        "        if not os.path.exists(output_path):\n",
        "            print('Creating output folder: ', output_path)\n",
        "            os.makedirs(output_path)\n",
        "        else:\n",
        "            print('Output folder already existed')\n",
        "\n",
        "        print('Saving features in ', output_path)\n",
        "        i = 0\n",
        "        t = time.time()\n",
        "\n",
        "        features_path = []\n",
        "        for filepath in self.dataset_df['filepath']:\n",
        "            if i % 100 == 0:\n",
        "                print('{} files processed in {}s'.format(i, time.time() - t))\n",
        "            audio_file, sample_rate = librosa.load(filepath, sr=sample_rate, res_type='kaiser_fast')\n",
        "            if mode == 'mfcc':\n",
        "                audio_features = self.compute_mfcc(audio_file, sample_rate, n_fft, hop_length, n_mfcc, deltas)\n",
        "            elif mode == 'stft':\n",
        "                audio_features = self.compute_stft(audio_file, sample_rate, n_fft, hop_length)\n",
        "            elif mode == 'mel-spectogram':\n",
        "                audio_features = self.compute_mel_spectogram(audio_file, sample_rate, n_fft, hop_length)\n",
        "\n",
        "            audio_features = np.pad(audio_features,\n",
        "                                    pad_width=((0, 0), (0, max_pad - audio_features.shape[1])))\n",
        "\n",
        "            save_path = os.path.join(output_path, filepath.split('/')[-1].replace('wav', 'npy'))\n",
        "            self.save_features(audio_features, save_path)\n",
        "            features_path.append(save_path)\n",
        "            i+=1\n",
        "        self.dataset_df['features_path'] = features_path\n",
        "        return self.dataset_df\n",
        "\n",
        "    @staticmethod\n",
        "    def save_features(audio_features, filepath):\n",
        "        np.save(filepath, audio_features)\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_mel_spectogram(audio_file, sample_rate, n_fft, hop_length):\n",
        "        return librosa.feature.melspectrogram(audio_file,\n",
        "                                              sr=sample_rate,\n",
        "                                              n_fft=n_fft,\n",
        "                                              hop_length=hop_length)\n",
        "    @staticmethod\n",
        "    def compute_stft(audio_file, sample_rate, n_fft, hop_length):\n",
        "        return librosa.stft(audio_file, n_fft=n_fft, hop_length=hop_length)\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_mfcc(audio_file, sample_rate, n_fft, hop_length, n_mfcc, deltas=False):\n",
        "        mfccs = librosa.feature.mfcc(audio_file,\n",
        "                                    sr=sample_rate,\n",
        "                                    n_fft=n_fft,\n",
        "                                    n_mfcc=n_mfcc,\n",
        "                                    )\n",
        "        # Change mode from interpolation to nearest\n",
        "        if deltas:\n",
        "          delta_mfccs = librosa.feature.delta(mfccs, mode='nearest')\n",
        "          delta2_mfccs = librosa.feature.delta(mfccs, order=2, mode='nearest')\n",
        "          return np.concatenate((mfccs, delta_mfccs, delta2_mfccs))\n",
        "        return mfccs"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzfg9qYAH0NC"
      },
      "source": [
        "# Create dataset and extract features\n",
        "fe = FeatureExtractor('UrbanSound8K/metadata/UrbanSound8K.csv')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4wZmY8n8od-"
      },
      "source": [
        "Access to disc and librosa loading of audio files is very slow on colab Notebook (30-40 min) we could load the pre-computed features instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "Y9fa-rfjH9rn",
        "outputId": "28fe0405-dbb5-460e-bac9-a2b410d461ab"
      },
      "source": [
        "# Uncomment and run to compute and save features on the colab notebook\n",
        "dataset_df = fe.compute_save_features(mode='mfcc', n_mfcc=13, output_path='features_mfcc', deltas=True)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Padding =  174\n",
            "Creating output folder:  features_mfcc\n",
            "Saving features in  features_mfcc\n",
            "0 files processed in 0.0002067089080810547s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-38134dc4ab84>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Uncomment and run to compute and save features on the colab notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_save_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mfcc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mfcc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'features_mfcc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-ec275522bcfa>\u001b[0m in \u001b[0;36mcompute_save_features\u001b[0;34m(self, mode, sample_rate, n_fft, hop_length, n_mfcc, output_path, deltas)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} files processed in {}s'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0maudio_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'kaiser_fast'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mfcc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0maudio_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mfcc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mfcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_sr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msr_native\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(y, orig_sr, target_sr, res_type, fix, scale, axis, **kwargs)\u001b[0m\n\u001b[1;32m    675\u001b[0m         )\n\u001b[1;32m    676\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresampy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_sr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lazy_loader/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__frame_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             raise ModuleNotFoundError(\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0;34mf\"No module named '{fd['spec']}'\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;34m\"This error is lazily reported, having originally occured in\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'resampy'\n\nThis error is lazily reported, having originally occured in\n  File /usr/local/lib/python3.10/dist-packages/librosa/core/audio.py, line 32, in <module>\n\n----> resampy = lazy.load(\"resampy\")",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9x6AFuuuQvb"
      },
      "source": [
        "# Unzip features\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1BU2B5EcbfyGBIOkB5YC44hpzPpuqw43H' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1BU2B5EcbfyGBIOkB5YC44hpzPpuqw43H\" -O features_mfcc.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip -q features_mfcc.zip\n",
        "!rm features_mfcc.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI_RjDQLBmpb"
      },
      "source": [
        "# Download dataset.json file\n",
        "!wget --no-check-certificate \"https://docs.google.com/uc?export=download&id=1pzSvGYaBXghLQFTZxlSex-Ts3T4B0X4C\" -O dataset.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Veb-Zxb5KEJD"
      },
      "source": [
        "dataset_df = pd.read_json('dataset.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giVHke5JwwYh"
      },
      "source": [
        "For the purpose of this experiment we will load all the data in memory and process it in minibatches. If we had computational resources and more time we could create Dataloader objects that would allow to perform many other operations as Data Augmentation and iterate faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMMTEYMoqks_"
      },
      "source": [
        "dataset_df['features'] = [np.asarray(np.load(feature_path)) for feature_path in dataset_df['features_path']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHyfHqtKzxAE"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "dataset_df['labels_categorical'] = [to_categorical(label, 10) for label in dataset_df['classID']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfDBipkQz1RF"
      },
      "source": [
        "dataset_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pQN4JBIxWeP"
      },
      "source": [
        "We are going to create splits for the train, validation and test sets of our dataset.\n",
        "For the purpose of the experiment and to make it quick we will use the sklearn function `train_test_split`, two times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-k-OKxXqkwh"
      },
      "source": [
        "# Split the dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Add one dimension for the channel\n",
        "X = np.array(dataset_df['features'].tolist())\n",
        "y = np.array(dataset_df['labels_categorical'].tolist())\n",
        "\n",
        "# As there is unbalance for some classes I am going to stratify it so we have the same proportion in train/test\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.30,\n",
        "                                                    random_state=1,\n",
        "                                                    stratify=y)\n",
        "# Create validation and test\n",
        "X_test, X_val, Y_test, Y_val = train_test_split(X_test,\n",
        "                                                Y_test,\n",
        "                                                test_size=0.5,\n",
        "                                                random_state=1,\n",
        "                                                stratify=Y_test)\n",
        "\n",
        "print(X_train.shape, X_val.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mbA_78g6iUa"
      },
      "source": [
        "## Machine Learning Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UXoQEZ16pjr"
      },
      "source": [
        "### Model Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-sygDIE29Og"
      },
      "source": [
        "We are going to create a **Fully Convolutional Network** Model using Keras running over Tensorflow with a few layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko1n0Tla3n1y"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y589P2Qm_AuG"
      },
      "source": [
        "As our images are rectangular in shape (y axis is MFCC, x axis is time), instead of using square filters (as usual) we are going to make them rectangular so they can learn better the correlation of the MFCCs with the temporal dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdM5w8r1kMgs"
      },
      "source": [
        "# FCN Model\n",
        "def create_model(num_classes=10, input_shape=None, dropout_ratio=None):\n",
        "    model = Sequential()\n",
        "    if input_shape is None:\n",
        "        model.add(Input(shape=(None, None, 1)))\n",
        "    else:\n",
        "        model.add(Input(shape=input_shape))\n",
        "    model.add(Conv2D(filters=16, kernel_size=(2, 4), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 3)))\n",
        "    model.add(Conv2D(filters=32, kernel_size=(2, 4), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=2))\n",
        "    model.add(Conv2D(filters=64, kernel_size=(2, 4), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=2))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(2, 4), activation='relu'))\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    if dropout_ratio is not None:\n",
        "        model.add(Dropout(dropout_ratio))\n",
        "    # Add dense linear layer\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj73VymN-ueo"
      },
      "source": [
        "As it is a multi classification problem we will use the **Categorical Cross Entropy loss**. As optimizer we will use the Keras implementation of **Adam** with the default hyperparameters values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vaj8BrhD4cyH"
      },
      "source": [
        "# Create and compile the model\n",
        "fcn_model = create_model(input_shape=X_train.shape[1:])\n",
        "fcn_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "fcn_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWTJXf4B6uNW"
      },
      "source": [
        "### Model training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBxKnyeo64TG"
      },
      "source": [
        "from keras.models import load_model\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTT0_wO27AFy"
      },
      "source": [
        "!mkdir saved_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1hJEl2f5JLI"
      },
      "source": [
        "def train_model(model, X_train, Y_train, X_val, Y_val, epochs, batch_size, callbacks):\n",
        "    model.fit(X_train,\n",
        "              Y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(X_val, Y_val),\n",
        "              callbacks=callbacks, verbose=1)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfzcdmYtETt3"
      },
      "source": [
        "We will create a checkpoint for **early stopping**, so we will select the model that performs better on the validation set.\n",
        "\n",
        "Creating a function to train the model will allow us to perform hyperparameter tuning faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nt9Q7KQ67WmV"
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath='saved_models/best_fcn.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
        "callbacks = [checkpointer]\n",
        "\n",
        "# Hyper-parameters\n",
        "epochs = 100\n",
        "batch_size = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IB-m4c17E7e"
      },
      "source": [
        "# Train the model\n",
        "model = train_model(model=fcn_model,\n",
        "                    X_train=X_train,\n",
        "                    X_val=X_val,\n",
        "                    Y_train=Y_train,\n",
        "                    Y_val=Y_val,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7lYwwrwDGey"
      },
      "source": [
        "# Load the best model\n",
        "best_model = load_model('saved_models/best_fcn.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvkVp34YEwRG"
      },
      "source": [
        "Looks like the model has overfitted to the training data towards the end of the training. We have selected the model that performed better on the validation set, saved by the checkpoint. The similarity between validation and test score tells us that our training methodology is correct and that our validation set is a good estimator of testing performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5a-BhlFDGj1"
      },
      "source": [
        "# Evaluating the model on the training and testing set\n",
        "score = best_model.evaluate(X_train, Y_train, verbose=0)\n",
        "print(\"Training Accuracy: \", score[1])\n",
        "\n",
        "score = best_model.evaluate(X_val, Y_val, verbose=0)\n",
        "print(\"Validation Accuracy: \", score[1])\n",
        "\n",
        "score = best_model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(\"Testing Accuracy: \", score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5wvOmYNBcZT"
      },
      "source": [
        "We see that there has been overfitting so we could train another model adding dropout before the last layer to add more regularization.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCA2qchv5JN9"
      },
      "source": [
        "# We add a dropout ratio of 0.25\n",
        "fcn_model = create_model(input_shape=X_train.shape[1:], dropout_ratio=0.5)\n",
        "fcn_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "fcn_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3YhwGSN5JRd"
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath='saved_models/best_fcn_dropout.hdf5', monitor='val_accuracy',\n",
        "                               verbose=1, save_best_only=True)\n",
        "callbacks = [checkpointer]\n",
        "\n",
        "model = train_model(model=fcn_model,\n",
        "                    X_train=X_train,\n",
        "                    X_val=X_val,\n",
        "                    Y_train=Y_train,\n",
        "                    Y_val=Y_val,\n",
        "                    epochs=200,\n",
        "                    batch_size=256,\n",
        "                    callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTyotHRpAMbB"
      },
      "source": [
        "best_model = load_model('saved_models/best_fcn_dropout.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UK4BTzRB_1Q"
      },
      "source": [
        "# Evaluating the model on the training and testing set\n",
        "score = best_model.evaluate(X_train, Y_train, verbose=0)\n",
        "print(\"Training Accuracy: \", score[1])\n",
        "\n",
        "score = best_model.evaluate(X_val, Y_val, verbose=0)\n",
        "print(\"Validation Accuracy: \", score[1])\n",
        "\n",
        "score = best_model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(\"Testing Accuracy: \", score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inagrMP3A4rB"
      },
      "source": [
        "# Plot a confusion matrix\n",
        "from sklearn import metrics\n",
        "Y_pred = best_model.predict(X_test)\n",
        "matrix = metrics.confusion_matrix(Y_test.argmax(axis=1), Y_pred.argmax(axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzTmmTg6A4oy"
      },
      "source": [
        "# Confusion matrix code (from https://github.com/triagemd/keras-eval/blob/master/keras_eval/visualizer.py)\n",
        "def plot_confusion_matrix(cm, concepts, normalize=False, show_text=True, fontsize=18, figsize=(16, 12),\n",
        "                          cmap=plt.cm.coolwarm_r, save_path=None, show_labels=True):\n",
        "    '''\n",
        "    Plot confusion matrix provided in 'cm'\n",
        "    Args:\n",
        "        cm: Confusion Matrix, square sized numpy array\n",
        "        concepts: Name of the categories to show\n",
        "        normalize: If True, normalize values between 0 and ones. Not valid if negative values.\n",
        "        show_text: If True, display cell values as text. Otherwise only display cell colors.\n",
        "        fontsize: Size of text\n",
        "        figsize: Size of figure\n",
        "        cmap: Color choice\n",
        "        save_path: If `save_path` specified, save confusion matrix in that location\n",
        "    Returns: Nothing. Plots confusion matrix\n",
        "    '''\n",
        "\n",
        "    if cm.ndim != 2 or cm.shape[0] != cm.shape[1]:\n",
        "        raise ValueError('Invalid confusion matrix shape, it should be square and ndim=2')\n",
        "\n",
        "    if cm.shape[0] != len(concepts) or cm.shape[1] != len(concepts):\n",
        "        raise ValueError('Number of concepts (%i) and dimensions of confusion matrix do not coincide (%i, %i)' %\n",
        "                         (len(concepts), cm.shape[0], cm.shape[1]))\n",
        "\n",
        "    plt.rcParams.update({'font.size': fontsize})\n",
        "\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm_normalized\n",
        "\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(cm, vmin=np.min(cm), vmax=np.max(cm), alpha=0.8, cmap=cmap)\n",
        "\n",
        "    fig.colorbar(cax)\n",
        "    ax.xaxis.tick_bottom()\n",
        "    plt.ylabel('True label', fontweight='bold')\n",
        "    plt.xlabel('Predicted label', fontweight='bold')\n",
        "\n",
        "    if show_labels:\n",
        "        n_labels = len(concepts)\n",
        "        ax.set_xticklabels(concepts)\n",
        "        ax.set_yticklabels(concepts)\n",
        "        plt.xticks(np.arange(0, n_labels, 1.0), rotation='vertical')\n",
        "        plt.yticks(np.arange(0, n_labels, 1.0))\n",
        "    else:\n",
        "        plt.axis('off')\n",
        "\n",
        "    if show_text:\n",
        "        # http://stackoverflow.com/questions/21712047/matplotlib-imshow-matshow-display-values-on-plot\n",
        "        min_val, max_val = 0, len(concepts)\n",
        "        ind_array = np.arange(min_val, max_val, 1.0)\n",
        "        x, y = np.meshgrid(ind_array, ind_array)\n",
        "        for i, (x_val, y_val) in enumerate(zip(x.flatten(), y.flatten())):\n",
        "            c = cm[int(x_val), int(y_val)]\n",
        "            ax.text(y_val, x_val, c, va='center', ha='center')\n",
        "\n",
        "    if save_path is not None:\n",
        "        plt.savefig(save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX4fa42o1zyW"
      },
      "source": [
        "To observe better the performance of the model and the mistakes made between different classes we plot the confusion matrix.\n",
        "\n",
        "In our case accuracy is a good metric because the dataset is mostly balanced but we observed a few classes with less samples (1`car_horn`, `gun_shot` and `siren`), so it will be good to observe the performance on these classes.\n",
        "\n",
        "We can observe that a lot of mistakes are happening between class `children_playing` and class `street_music` so maybe it will be worth it to spend a little bit more time doing analysis and finding what could be the reasons.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WabP5FBC8ph"
      },
      "source": [
        "class_dictionary = {3: 'dog_bark', 2: 'children_playing', 1: 'car_horn', 0: 'air_conditioner', 9: 'street_music', 6: 'gun_shot', 8: 'siren', 5: 'engine_idling', 7: 'jackhammer', 4: 'drilling'}\n",
        "classes = [class_dictionary[key] for key in sorted(class_dictionary.keys())]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIsWLoxGHHOv"
      },
      "source": [
        "plot_confusion_matrix(matrix, classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12pnVV0xLiBr"
      },
      "source": [
        "## Conclusions\n",
        "\n",
        "We can observe a bump of 1-2% in the test set accuracy when introducing dropout as regularization. This shows that it has been a successful addition to our model.\n",
        "\n",
        "There are many things that we can try to improve the model's performance such as:\n",
        "\n",
        "- Hyperparameter tuning:\n",
        "  - Tuning the parameters of feature extraction\n",
        "  - Tuning the network parameters (number of layers, pooling layers, number and filter shape...)\n",
        "  - Tuning the network hyperparameters (Learning rate, optimizer)\n",
        "\n",
        "- Feature extraction:\n",
        "  - Use STFT: The raw spectogram could provide more information to the CNN to learn correlation between frequency and time than the MFCCs.\n",
        "  - Use Mel-Spectogram: The mel-spectogram could provide more information to the CNN to learn correlation between frequency and time than the MFCCs."
      ]
    }
  ]
}