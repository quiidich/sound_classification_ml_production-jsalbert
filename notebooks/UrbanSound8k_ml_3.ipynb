{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UrbanSound8k_machine_learning.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAR2KCeZGVX3"
      },
      "source": [
        "# Applying Machine Learning on UrbanSound8k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USPAMetFGQ7O"
      },
      "source": [
        "## Install Packages\n",
        "\n",
        "We install:\n",
        "- Machine learning libraries: `Keras`, `sklearn`\n",
        "- Audio processing: `librosa`\n",
        "- Plots: `Plotly`, `matplotlib`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sefSvCUtzKfx"
      },
      "source": [
        "!pip install pandas\n",
        "!pip install setuptools\n",
        "!pip install numpy\n",
        "!pip install sklearn\n",
        "!pip install librosa\n",
        "!pip install plotly\n",
        "!pip install matplotlib\n",
        "!pip install pillow\n",
        "!pip install keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrVjHhS40MZM"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import librosa\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NA_prGkx8N7P",
        "outputId": "09d6f400-b21d-4883-e655-3de12cefa5c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd gdrive/MyDrive/urbansound8k\n",
        "!wget https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz -O urban8k.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoPrdeZSQn6H",
        "outputId": "b025060f-f830-41a4-e8e7-f7cda581cdfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "--2023-11-27 02:20:52--  https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.79.172, 188.184.98.238, 188.184.103.159, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.79.172|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/1203745/files/UrbanSound8K.tar.gz [following]\n",
            "--2023-11-27 02:20:52--  https://zenodo.org/records/1203745/files/UrbanSound8K.tar.gz\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6023741708 (5.6G) [application/octet-stream]\n",
            "Saving to: ‘urban8k.tgz’\n",
            "\n",
            "urban8k.tgz         100%[===================>]   5.61G  18.9MB/s    in 3m 54s  \n",
            "\n",
            "2023-11-27 02:24:47 (24.5 MB/s) - ‘urban8k.tgz’ saved [6023741708/6023741708]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ELY9HvKQiwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz -O urban8k.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IdovGFEPado",
        "outputId": "a800e4ae-e3c5-4199-c12a-452052b18818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-27 00:52:26--  https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.79.172, 188.184.98.238, 188.184.103.159, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.79.172|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/1203745/files/UrbanSound8K.tar.gz [following]\n",
            "--2023-11-27 00:52:26--  https://zenodo.org/records/1203745/files/UrbanSound8K.tar.gz\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6023741708 (5.6G) [application/octet-stream]\n",
            "Saving to: ‘urban8k.tgz’\n",
            "\n",
            "urban8k.tgz         100%[===================>]   5.61G  21.9MB/s    in 4m 35s  \n",
            "\n",
            "2023-11-27 00:57:01 (20.9 MB/s) - ‘urban8k.tgz’ saved [6023741708/6023741708]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "try:\n",
        "    dataset_path = '/content/gdrive/MyDrive/urbansound8k'\n",
        "    shutil.copy('urban8k.tgz', os.path.join(dataset_path, 'urban8k.tgz'))\n",
        "except Exception as err:\n",
        "    print(str(err))\n",
        "\n",
        "#!tar -xzf urban8k.tgz\n",
        "#!rm urban8k.tgz"
      ],
      "metadata": {
        "id": "MA2Na1D1_LhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ikg2sdn36H0"
      },
      "source": [
        "# # Unzip dataset\n",
        "# !wget https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz -O urban8k.tgz\n",
        "!tar -xzf urban8k.tgz\n",
        "# !rm urban8k.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOx82BdSom6q"
      },
      "source": [
        "## Design Choices and Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YbZ0ZqyosFi"
      },
      "source": [
        "After analysing the dataset and spending a bit of time reading about state-of-the-art on audio signal classification, and some of my [previous work](https://github.com/jsalbert/Music-Genre-Classification-with-Deep-Learning) I have made the following design choices and proposals:\n",
        "\n",
        "Train a Convolutional Neural Network and use either MFCCs, STFT or Mel-Spectogram as input.\n",
        "\n",
        "- As the audios duration range from 0 to 4s, I pad the spectogram generated, to make all the audios of equal length.\n",
        "\n",
        "Feature options:\n",
        "\n",
        "- Using MFCCs as features:\n",
        "  - It is usual to compute the first 13 MFCCs, their derivatives and second derivatives and use it as features.\n",
        "  - Or it is also usual to use 40 MFCCs as it is the Librosa default.\n",
        "\n",
        "- Using the STFT as features:\n",
        "  - Contains less human processing than MFCCs and Mel-Spectogram, the CNN could learn other filters rather than the representations designed by humans.\n",
        "\n",
        "- Using Mel-Spectogram as features:\n",
        "  - A transformation applied on the STFT that approximates how humans perceive the sound. Less human engineered than MFCCs but a bit more than STFT.\n",
        "\n",
        "My first choice would be using STFT and Mel-Spectogram as it looks that CNNs could take more advantage of the frequency-temporal structure but due to **computational resources** and limited time I will show the use **MFCCs** as features as they are much more memory efficient.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctXVt_OEsrie"
      },
      "source": [
        "## Dataset Preprocessing and Splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J728zt898gNW"
      },
      "source": [
        "I load all the audio data using Librosa and the default sample rate of 22050Hz. This design decision is based on\n",
        "([Source]((https://librosa.org/blog/2019/07/17/resample-on-load/#Okay...-but-why-22050-Hz?--Why-not-44100-or-48000?))) and in further experiments different sample rates could be tried.\n",
        "\n",
        "> Humans can hear up to around 20000 Hz, it's possible to successfully analyze music and speech data at much lower rates without sacrificing much. The highest pitches we usually care about detecting are around C9≈8372 Hz, well below the 11025 cutoff implied by fs=22050.\n",
        "\n",
        "By default Librosa will load the audio in mono, giving us 1 channel.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPbaPrONsoiY"
      },
      "source": [
        "# FeatureExtractor class including librosa audio processing functions\n",
        "class FeatureExtractor:\n",
        "    def __init__(self, csv_file):\n",
        "        self.csv_file = csv_file\n",
        "        self.max_audio_duration = 4\n",
        "        self.dataset_df = self._create_dataset(csv_file)\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_dataset(csv_file):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataset_path: path with the .wav files after unzipping\n",
        "        Returns: A pandas dataframe with the list of files and labels (`filenames`, `labels`)\n",
        "        \"\"\"\n",
        "        dataset_df = pd.read_csv(csv_file)\n",
        "        filepaths = []\n",
        "        for i, row in dataset_df.iterrows():\n",
        "            filepaths.append(os.path.join('UrbanSound8K/audio', 'fold'+str(row['fold']), row['slice_file_name']))\n",
        "        dataset_df['filepath'] = filepaths\n",
        "        return dataset_df\n",
        "\n",
        "    @staticmethod\n",
        "    def _compute_max_pad_length(max_audio_length, sample_rate=22050, n_fft=2048, hop_length=512):\n",
        "        dummy_file = np.random.random(max_audio_length*sample_rate)\n",
        "        stft = librosa.stft(dummy_file, n_fft=n_fft, hop_length=hop_length)\n",
        "        # Return an even number for CNN computation purposes\n",
        "        if stft.shape[1] % 2 != 0:\n",
        "            return stft.shape[1] + 1\n",
        "        return stft.shape[1]\n",
        "\n",
        "    def compute_save_features(self,\n",
        "                        mode='mfcc',\n",
        "                        sample_rate=22050,\n",
        "                        n_fft=2048,\n",
        "                        hop_length=512,\n",
        "                        n_mfcc=40,\n",
        "                        output_path='features',\n",
        "                        deltas=False\n",
        "                        ):\n",
        "        dataset_features = []\n",
        "        max_pad = self._compute_max_pad_length(self.max_audio_duration,\n",
        "                                               sample_rate=sample_rate,\n",
        "                                               n_fft=n_fft,\n",
        "                                               hop_length=hop_length)\n",
        "        print('Max Padding = ', max_pad)\n",
        "\n",
        "        if not os.path.exists(output_path):\n",
        "            print('Creating output folder: ', output_path)\n",
        "            os.makedirs(output_path)\n",
        "        else:\n",
        "            print('Output folder already existed')\n",
        "\n",
        "        print('Saving features in ', output_path)\n",
        "        i = 0\n",
        "        t = time.time()\n",
        "\n",
        "        features_path = []\n",
        "        for filepath in self.dataset_df['filepath']:\n",
        "            if i % 100 == 0:\n",
        "                print('{} files processed in {}s'.format(i, time.time() - t))\n",
        "            audio_file, sample_rate = librosa.load(filepath, sr=sample_rate, res_type='kaiser_fast')\n",
        "            if mode == 'mfcc':\n",
        "                audio_features = self.compute_mfcc(audio_file, sample_rate, n_fft, hop_length, n_mfcc, deltas)\n",
        "            elif mode == 'stft':\n",
        "                audio_features = self.compute_stft(audio_file, sample_rate, n_fft, hop_length)\n",
        "            elif mode == 'mel-spectogram':\n",
        "                audio_features = self.compute_mel_spectogram(audio_file, sample_rate, n_fft, hop_length)\n",
        "\n",
        "            audio_features = np.pad(audio_features,\n",
        "                                    pad_width=((0, 0), (0, max_pad - audio_features.shape[1])))\n",
        "\n",
        "            save_path = os.path.join(output_path, filepath.split('/')[-1].replace('wav', 'npy'))\n",
        "            self.save_features(audio_features, save_path)\n",
        "            features_path.append(save_path)\n",
        "            i+=1\n",
        "        self.dataset_df['features_path'] = features_path\n",
        "        return self.dataset_df\n",
        "\n",
        "    @staticmethod\n",
        "    def save_features(audio_features, filepath):\n",
        "        np.save(filepath, audio_features)\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_mel_spectogram(audio_file, sample_rate, n_fft, hop_length):\n",
        "        return librosa.feature.melspectrogram(y=audio_file,\n",
        "                                              sr=sample_rate,\n",
        "                                              n_fft=n_fft,\n",
        "                                              hop_length=hop_length)\n",
        "    @staticmethod\n",
        "    def compute_stft(audio_file, sample_rate, n_fft, hop_length):\n",
        "        return librosa.stft(y=audio_file, n_fft=n_fft, hop_length=hop_length)\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_mfcc(audio_file, sample_rate, n_fft, hop_length, n_mfcc, deltas=False):\n",
        "        mfccs = librosa.feature.mfcc(y=audio_file,\n",
        "                                    sr=sample_rate,\n",
        "                                    n_fft=n_fft,\n",
        "                                    n_mfcc=n_mfcc,\n",
        "                                    )\n",
        "        # Change mode from interpolation to nearest\n",
        "        if deltas:\n",
        "          delta_mfccs = librosa.feature.delta(mfccs, mode='nearest')\n",
        "          delta2_mfccs = librosa.feature.delta(mfccs, order=2, mode='nearest')\n",
        "          return np.concatenate((mfccs, delta_mfccs, delta2_mfccs))\n",
        "        return mfccs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzfg9qYAH0NC"
      },
      "source": [
        "# Create dataset and extract features\n",
        "fe = FeatureExtractor('UrbanSound8K/metadata/UrbanSound8K.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install resampy\n",
        "!pip install samplerate\n",
        "import resampy\n",
        "import samplerate"
      ],
      "metadata": {
        "id": "q0tK75_FhmeS",
        "outputId": "c35b73ea-6e0c-4852-93e2-73cfb20f5bc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting resampy\n",
            "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from resampy) (1.23.5)\n",
            "Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.10/dist-packages (from resampy) (0.58.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.53->resampy) (0.41.1)\n",
            "Installing collected packages: resampy\n",
            "Successfully installed resampy-0.4.2\n",
            "Collecting samplerate\n",
            "  Downloading samplerate-0.1.0-py2.py3-none-any.whl (4.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from samplerate) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from samplerate) (1.23.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.0->samplerate) (2.21)\n",
            "Installing collected packages: samplerate\n",
            "Successfully installed samplerate-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4wZmY8n8od-"
      },
      "source": [
        "Access to disc and librosa loading of audio files is very slow on colab Notebook (30-40 min) we could load the pre-computed features instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9fa-rfjH9rn",
        "outputId": "dc205ef3-8a7b-4079-f513-87edf74ca7e7"
      },
      "source": [
        "# Uncomment and run to compute and save features on the colab notebook\n",
        "dataset_df = fe.compute_save_features(mode='mfcc', n_mfcc=13, output_path='features_mfcc', deltas=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Padding =  174\n",
            "Creating output folder:  features_mfcc\n",
            "Saving features in  features_mfcc\n",
            "0 files processed in 0.0007746219635009766s\n",
            "100 files processed in 45.99389028549194s\n",
            "200 files processed in 62.50243330001831s\n",
            "300 files processed in 77.38388705253601s\n",
            "400 files processed in 89.4657723903656s\n",
            "500 files processed in 100.07711029052734s\n",
            "600 files processed in 112.72688508033752s\n",
            "700 files processed in 126.03003811836243s\n",
            "800 files processed in 141.22181677818298s\n",
            "900 files processed in 152.72897672653198s\n",
            "1000 files processed in 164.46209192276s\n",
            "1100 files processed in 174.41947484016418s\n",
            "1200 files processed in 186.48194360733032s\n",
            "1300 files processed in 205.7965602874756s\n",
            "1400 files processed in 229.11125469207764s\n",
            "1500 files processed in 242.97292613983154s\n",
            "1600 files processed in 261.04434871673584s\n",
            "1700 files processed in 271.28188729286194s\n",
            "1800 files processed in 285.0249991416931s\n",
            "1900 files processed in 296.3486907482147s\n",
            "2000 files processed in 306.00324058532715s\n",
            "2100 files processed in 317.54646587371826s\n",
            "2200 files processed in 328.4575619697571s\n",
            "2300 files processed in 340.1612460613251s\n",
            "2400 files processed in 352.2323064804077s\n",
            "2500 files processed in 361.1265094280243s\n",
            "2600 files processed in 372.70140862464905s\n",
            "2700 files processed in 383.87235140800476s\n",
            "2800 files processed in 395.8372781276703s\n",
            "2900 files processed in 418.1953401565552s\n",
            "3000 files processed in 443.2039444446564s\n",
            "3100 files processed in 463.30876302719116s\n",
            "3200 files processed in 474.65972805023193s\n",
            "3300 files processed in 487.01388454437256s\n",
            "3400 files processed in 499.18951177597046s\n",
            "3500 files processed in 514.2237746715546s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/librosa/core/spectrum.py:257: UserWarning: n_fft=2048 is too large for input signal of length=1323\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3600 files processed in 527.8345155715942s\n",
            "3700 files processed in 538.3808953762054s\n",
            "3800 files processed in 550.4247434139252s\n",
            "3900 files processed in 564.0537965297699s\n",
            "4000 files processed in 575.6863393783569s\n",
            "4100 files processed in 588.2497456073761s\n",
            "4200 files processed in 600.1979653835297s\n",
            "4300 files processed in 612.5027878284454s\n",
            "4400 files processed in 641.7524988651276s\n",
            "4500 files processed in 658.2967164516449s\n",
            "4600 files processed in 673.7819590568542s\n",
            "4700 files processed in 691.7928495407104s\n",
            "4800 files processed in 702.8438184261322s\n",
            "4900 files processed in 716.2327461242676s\n",
            "5000 files processed in 731.0906686782837s\n",
            "5100 files processed in 741.286881685257s\n",
            "5200 files processed in 753.6484916210175s\n",
            "5300 files processed in 766.1632430553436s\n",
            "5400 files processed in 776.975147485733s\n",
            "5500 files processed in 789.0384595394135s\n",
            "5600 files processed in 800.2216320037842s\n",
            "5700 files processed in 810.180867433548s\n",
            "5800 files processed in 824.6807942390442s\n",
            "5900 files processed in 840.3215622901917s\n",
            "6000 files processed in 850.1400651931763s\n",
            "6100 files processed in 867.1620700359344s\n",
            "6200 files processed in 880.3567714691162s\n",
            "6300 files processed in 893.7206473350525s\n",
            "6400 files processed in 904.8541917800903s\n",
            "6500 files processed in 916.0263092517853s\n",
            "6600 files processed in 926.8672587871552s\n",
            "6700 files processed in 938.8685419559479s\n",
            "6800 files processed in 949.3749723434448s\n",
            "6900 files processed in 961.8270919322968s\n",
            "7000 files processed in 974.2124104499817s\n",
            "7100 files processed in 984.2013666629791s\n",
            "7200 files processed in 997.1693594455719s\n",
            "7300 files processed in 1008.3580069541931s\n",
            "7400 files processed in 1016.9284982681274s\n",
            "7500 files processed in 1031.757131576538s\n",
            "7600 files processed in 1046.87069773674s\n",
            "7700 files processed in 1062.7198717594147s\n",
            "7800 files processed in 1100.5940754413605s\n",
            "7900 files processed in 1127.990071773529s\n",
            "8000 files processed in 1137.8510434627533s\n",
            "8100 files processed in 1152.00417137146s\n",
            "8200 files processed in 1164.891194820404s\n",
            "8300 files processed in 1173.8547794818878s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/librosa/core/spectrum.py:257: UserWarning: n_fft=2048 is too large for input signal of length=1103\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/spectrum.py:257: UserWarning: n_fft=2048 is too large for input signal of length=1523\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8400 files processed in 1185.257384300232s\n",
            "8500 files processed in 1200.9792277812958s\n",
            "8600 files processed in 1218.2197844982147s\n",
            "8700 files processed in 1231.6960456371307s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9x6AFuuuQvb"
      },
      "source": [
        "# Unzip features\n",
        "#!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1BU2B5EcbfyGBIOkB5YC44hpzPpuqw43H' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1BU2B5EcbfyGBIOkB5YC44hpzPpuqw43H\" -O features_mfcc.zip && rm -rf /tmp/cookies.txt\n",
        "#!unzip -q features_mfcc.zip\n",
        "#!rm features_mfcc.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI_RjDQLBmpb"
      },
      "source": [
        "# Download dataset.json file\n",
        "#!wget --no-check-certificate \"https://docs.google.com/uc?export=download&id=1pzSvGYaBXghLQFTZxlSex-Ts3T4B0X4C\" -O dataset.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Veb-Zxb5KEJD"
      },
      "source": [
        "#dataset_df = pd.read_json('dataset.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giVHke5JwwYh"
      },
      "source": [
        "For the purpose of this experiment we will load all the data in memory and process it in minibatches. If we had computational resources and more time we could create Dataloader objects that would allow to perform many other operations as Data Augmentation and iterate faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMMTEYMoqks_"
      },
      "source": [
        "dataset_df['features'] = [np.asarray(np.load(feature_path)) for feature_path in dataset_df['features_path']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHyfHqtKzxAE"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "dataset_df['labels_categorical'] = [to_categorical(label, 10) for label in dataset_df['classID']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfDBipkQz1RF"
      },
      "source": [
        "dataset_df.head()\n",
        "\n",
        "import pickle\n",
        "with open('dataset_df.pickle','wb') as f:\n",
        "     pickle.dump(dataset_df, f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.copy('dataset_df.pickle', '/content/gdrive/MyDrive/urbansound8k')"
      ],
      "metadata": {
        "id": "8Ccc_VNptwBb",
        "outputId": "20de6f88-c3aa-46c7-948a-f669c947fac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/urbansound8k/dataset_df.pickle'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pQN4JBIxWeP"
      },
      "source": [
        "We are going to create splits for the train, validation and test sets of our dataset.\n",
        "For the purpose of the experiment and to make it quick we will use the sklearn function `train_test_split`, two times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-k-OKxXqkwh"
      },
      "source": [
        "# Split the dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Add one dimension for the channel\n",
        "X = np.array(dataset_df['features'].tolist())\n",
        "y = np.array(dataset_df['labels_categorical'].tolist())\n",
        "\n",
        "# As there is unbalance for some classes I am going to stratify it so we have the same proportion in train/test\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.30,\n",
        "                                                    random_state=1,\n",
        "                                                    stratify=y)\n",
        "# Create validation and test\n",
        "X_test, X_val, Y_test, Y_val = train_test_split(X_test,\n",
        "                                                Y_test,\n",
        "                                                test_size=0.5,\n",
        "                                                random_state=1,\n",
        "                                                stratify=Y_test)\n",
        "\n",
        "print(X_train.shape, X_val.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mbA_78g6iUa"
      },
      "source": [
        "## Machine Learning Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UXoQEZ16pjr"
      },
      "source": [
        "### Model Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-sygDIE29Og"
      },
      "source": [
        "We are going to create a **Fully Convolutional Network** Model using Keras running over Tensorflow with a few layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko1n0Tla3n1y"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y589P2Qm_AuG"
      },
      "source": [
        "As our images are rectangular in shape (y axis is MFCC, x axis is time), instead of using square filters (as usual) we are going to make them rectangular so they can learn better the correlation of the MFCCs with the temporal dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdM5w8r1kMgs"
      },
      "source": [
        "# FCN Model\n",
        "def create_model(num_classes=10, input_shape=None, dropout_ratio=None):\n",
        "    model = Sequential()\n",
        "    if input_shape is None:\n",
        "        model.add(Input(shape=(None, None, 1)))\n",
        "    else:\n",
        "        model.add(Input(shape=input_shape))\n",
        "    model.add(Conv2D(filters=16, kernel_size=(2, 4), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 3)))\n",
        "    model.add(Conv2D(filters=32, kernel_size=(2, 4), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=2))\n",
        "    model.add(Conv2D(filters=64, kernel_size=(2, 4), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=2))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(2, 4), activation='relu'))\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    if dropout_ratio is not None:\n",
        "        model.add(Dropout(dropout_ratio))\n",
        "    # Add dense linear layer\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj73VymN-ueo"
      },
      "source": [
        "As it is a multi classification problem we will use the **Categorical Cross Entropy loss**. As optimizer we will use the Keras implementation of **Adam** with the default hyperparameters values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vaj8BrhD4cyH"
      },
      "source": [
        "# Create and compile the model\n",
        "fcn_model = create_model(input_shape=X_train.shape[1:])\n",
        "fcn_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "fcn_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWTJXf4B6uNW"
      },
      "source": [
        "### Model training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBxKnyeo64TG"
      },
      "source": [
        "from keras.models import load_model\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTT0_wO27AFy"
      },
      "source": [
        "!mkdir saved_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1hJEl2f5JLI"
      },
      "source": [
        "def train_model(model, X_train, Y_train, X_val, Y_val, epochs, batch_size, callbacks):\n",
        "    model.fit(X_train,\n",
        "              Y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(X_val, Y_val),\n",
        "              callbacks=callbacks, verbose=1)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfzcdmYtETt3"
      },
      "source": [
        "We will create a checkpoint for **early stopping**, so we will select the model that performs better on the validation set.\n",
        "\n",
        "Creating a function to train the model will allow us to perform hyperparameter tuning faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nt9Q7KQ67WmV"
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath='saved_models/best_fcn.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
        "callbacks = [checkpointer]\n",
        "\n",
        "# Hyper-parameters\n",
        "epochs = 100\n",
        "batch_size = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IB-m4c17E7e"
      },
      "source": [
        "# Train the model\n",
        "model = train_model(model=fcn_model,\n",
        "                    X_train=X_train,\n",
        "                    X_val=X_val,\n",
        "                    Y_train=Y_train,\n",
        "                    Y_val=Y_val,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7lYwwrwDGey"
      },
      "source": [
        "# Load the best model\n",
        "best_model = load_model('saved_models/best_fcn.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvkVp34YEwRG"
      },
      "source": [
        "Looks like the model has overfitted to the training data towards the end of the training. We have selected the model that performed better on the validation set, saved by the checkpoint. The similarity between validation and test score tells us that our training methodology is correct and that our validation set is a good estimator of testing performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5a-BhlFDGj1"
      },
      "source": [
        "# Evaluating the model on the training and testing set\n",
        "score = best_model.evaluate(X_train, Y_train, verbose=0)\n",
        "print(\"Training Accuracy: \", score[1])\n",
        "\n",
        "score = best_model.evaluate(X_val, Y_val, verbose=0)\n",
        "print(\"Validation Accuracy: \", score[1])\n",
        "\n",
        "score = best_model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(\"Testing Accuracy: \", score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5wvOmYNBcZT"
      },
      "source": [
        "We see that there has been overfitting so we could train another model adding dropout before the last layer to add more regularization.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCA2qchv5JN9"
      },
      "source": [
        "# We add a dropout ratio of 0.25\n",
        "fcn_model = create_model(input_shape=X_train.shape[1:], dropout_ratio=0.5)\n",
        "fcn_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "fcn_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3YhwGSN5JRd"
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath='saved_models/best_fcn_dropout.hdf5', monitor='val_accuracy',\n",
        "                               verbose=1, save_best_only=True)\n",
        "callbacks = [checkpointer]\n",
        "\n",
        "model = train_model(model=fcn_model,\n",
        "                    X_train=X_train,\n",
        "                    X_val=X_val,\n",
        "                    Y_train=Y_train,\n",
        "                    Y_val=Y_val,\n",
        "                    epochs=200,\n",
        "                    batch_size=256,\n",
        "                    callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTyotHRpAMbB"
      },
      "source": [
        "best_model = load_model('saved_models/best_fcn_dropout.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UK4BTzRB_1Q"
      },
      "source": [
        "# Evaluating the model on the training and testing set\n",
        "score = best_model.evaluate(X_train, Y_train, verbose=0)\n",
        "print(\"Training Accuracy: \", score[1])\n",
        "\n",
        "score = best_model.evaluate(X_val, Y_val, verbose=0)\n",
        "print(\"Validation Accuracy: \", score[1])\n",
        "\n",
        "score = best_model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(\"Testing Accuracy: \", score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inagrMP3A4rB"
      },
      "source": [
        "# Plot a confusion matrix\n",
        "from sklearn import metrics\n",
        "Y_pred = best_model.predict(X_test)\n",
        "matrix = metrics.confusion_matrix(Y_test.argmax(axis=1), Y_pred.argmax(axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzTmmTg6A4oy"
      },
      "source": [
        "# Confusion matrix code (from https://github.com/triagemd/keras-eval/blob/master/keras_eval/visualizer.py)\n",
        "def plot_confusion_matrix(cm, concepts, normalize=False, show_text=True, fontsize=18, figsize=(16, 12),\n",
        "                          cmap=plt.cm.coolwarm_r, save_path=None, show_labels=True):\n",
        "    '''\n",
        "    Plot confusion matrix provided in 'cm'\n",
        "    Args:\n",
        "        cm: Confusion Matrix, square sized numpy array\n",
        "        concepts: Name of the categories to show\n",
        "        normalize: If True, normalize values between 0 and ones. Not valid if negative values.\n",
        "        show_text: If True, display cell values as text. Otherwise only display cell colors.\n",
        "        fontsize: Size of text\n",
        "        figsize: Size of figure\n",
        "        cmap: Color choice\n",
        "        save_path: If `save_path` specified, save confusion matrix in that location\n",
        "    Returns: Nothing. Plots confusion matrix\n",
        "    '''\n",
        "\n",
        "    if cm.ndim != 2 or cm.shape[0] != cm.shape[1]:\n",
        "        raise ValueError('Invalid confusion matrix shape, it should be square and ndim=2')\n",
        "\n",
        "    if cm.shape[0] != len(concepts) or cm.shape[1] != len(concepts):\n",
        "        raise ValueError('Number of concepts (%i) and dimensions of confusion matrix do not coincide (%i, %i)' %\n",
        "                         (len(concepts), cm.shape[0], cm.shape[1]))\n",
        "\n",
        "    plt.rcParams.update({'font.size': fontsize})\n",
        "\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm_normalized\n",
        "\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(cm, vmin=np.min(cm), vmax=np.max(cm), alpha=0.8, cmap=cmap)\n",
        "\n",
        "    fig.colorbar(cax)\n",
        "    ax.xaxis.tick_bottom()\n",
        "    plt.ylabel('True label', fontweight='bold')\n",
        "    plt.xlabel('Predicted label', fontweight='bold')\n",
        "\n",
        "    if show_labels:\n",
        "        n_labels = len(concepts)\n",
        "        ax.set_xticklabels(concepts)\n",
        "        ax.set_yticklabels(concepts)\n",
        "        plt.xticks(np.arange(0, n_labels, 1.0), rotation='vertical')\n",
        "        plt.yticks(np.arange(0, n_labels, 1.0))\n",
        "    else:\n",
        "        plt.axis('off')\n",
        "\n",
        "    if show_text:\n",
        "        # http://stackoverflow.com/questions/21712047/matplotlib-imshow-matshow-display-values-on-plot\n",
        "        min_val, max_val = 0, len(concepts)\n",
        "        ind_array = np.arange(min_val, max_val, 1.0)\n",
        "        x, y = np.meshgrid(ind_array, ind_array)\n",
        "        for i, (x_val, y_val) in enumerate(zip(x.flatten(), y.flatten())):\n",
        "            c = cm[int(x_val), int(y_val)]\n",
        "            ax.text(y_val, x_val, c, va='center', ha='center')\n",
        "\n",
        "    if save_path is not None:\n",
        "        plt.savefig(save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX4fa42o1zyW"
      },
      "source": [
        "To observe better the performance of the model and the mistakes made between different classes we plot the confusion matrix.\n",
        "\n",
        "In our case accuracy is a good metric because the dataset is mostly balanced but we observed a few classes with less samples (1`car_horn`, `gun_shot` and `siren`), so it will be good to observe the performance on these classes.\n",
        "\n",
        "We can observe that a lot of mistakes are happening between class `children_playing` and class `street_music` so maybe it will be worth it to spend a little bit more time doing analysis and finding what could be the reasons.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WabP5FBC8ph"
      },
      "source": [
        "class_dictionary = {3: 'dog_bark', 2: 'children_playing', 1: 'car_horn', 0: 'air_conditioner', 9: 'street_music', 6: 'gun_shot', 8: 'siren', 5: 'engine_idling', 7: 'jackhammer', 4: 'drilling'}\n",
        "classes = [class_dictionary[key] for key in sorted(class_dictionary.keys())]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIsWLoxGHHOv"
      },
      "source": [
        "plot_confusion_matrix(matrix, classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12pnVV0xLiBr"
      },
      "source": [
        "## Conclusions\n",
        "\n",
        "We can observe a bump of 1-2% in the test set accuracy when introducing dropout as regularization. This shows that it has been a successful addition to our model.\n",
        "\n",
        "There are many things that we can try to improve the model's performance such as:\n",
        "\n",
        "- Hyperparameter tuning:\n",
        "  - Tuning the parameters of feature extraction\n",
        "  - Tuning the network parameters (number of layers, pooling layers, number and filter shape...)\n",
        "  - Tuning the network hyperparameters (Learning rate, optimizer)\n",
        "\n",
        "- Feature extraction:\n",
        "  - Use STFT: The raw spectogram could provide more information to the CNN to learn correlation between frequency and time than the MFCCs.\n",
        "  - Use Mel-Spectogram: The mel-spectogram could provide more information to the CNN to learn correlation between frequency and time than the MFCCs."
      ]
    }
  ]
}